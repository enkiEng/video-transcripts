---
title: Nvidia, You're Late. World's First 128GB LLM Mini Is Here!
source: youtube
url: https://www.youtube.com/watch?v=B7GDr-VFuEo
video_id: B7GDr-VFuEo
channel: Alex Ziskind
author: Alex Ziskind
date_published: 2025-06-10
date_retrieved: 2025-11-05
duration: 20 minutes
tags:
  - LLM
  - mini-pc
  - AMD
  - Ryzen
  - benchmarks
  - hardware-review
  - GMKTech
  - AI
  - machine-learning
  - memory-bandwidth
  - performance-comparison
  - Mac-Mini
  - unified-memory
  - local-AI
  - developer-tools
  - Nvidia
  - DGX-Spark
description: In-depth review and testing of the GMKTec EVO-X2 mini PC with AMD Ryzen AI Max Plus 395 featuring 128GB RAM for local LLM workloads. Includes extensive performance comparisons with M4 Mac Mini, M4 Pro Mac Mini, memory bandwidth analysis, and discussion of static partitioning vs unified memory architecture. Explores why Nvidia's DGX Spark delay makes this mini PC a compelling alternative.
---

# Nvidia, You're Late. World's First 128GB LLM Mini Is Here!

## Metadata
- **Source**: YouTube
- **Channel/Author**: Alex Ziskind
- **Published**: 2025-06-10
- **URL**: https://www.youtube.com/watch?v=B7GDr-VFuEo

## Summary

Alex Ziskind reviews the GMKTec EVO-X2 mini PC featuring AMD's Ryzen AI Max Plus 395 chip with 128GB RAM, positioning it as an alternative to the delayed Nvidia DGX Spark. Key highlights include:

- **Specifications**: 16 cores/32 threads up to 5.1GHz, Radeon 8060S GPU (rival to RTX 4060), 128GB LPDDR5X RAM @ 8000MHz, dual PCIe Gen 4 SSD slots
- **Connectivity**: 2x USB 4, 5x USB-A, SD card reader, HDMI, DisplayPort, 2.5Gbps Ethernet, Wi-Fi 7
- **LLM Performance Testing**: Extensive benchmarks on models from 1B to 70B parameters
  - Qwen 32B Q4: 10.58 tokens/sec
  - Llama 3.3 70B Q4: 5.1 tokens/sec (39GB model)
  - DeepSeek R1 Distill 7B Q4: 43 tokens/sec
- **Memory Bandwidth Analysis**:
  - AMD Ryzen AI Max Plus: 256 GB/s (measured 120 GB/s via Stream benchmark)
  - M4 Mac Mini: 120 GB/s (measured 96 GB/s)
  - M4 Pro Mac Mini: 273 GB/s (measured 209 GB/s)
  - Nvidia DGX Spark: 279 GB/s (expected)
- **Comparison Results**: Outperforms M4 Mac Mini significantly, roughly matches M4 Pro Mac Mini in many workloads
- **Architecture**: Static partitioning (64GB/64GB split) vs Apple's unified memory
- **Future Outlook**: ROCm support coming for AMD chip could significantly improve performance
- **Pricing**: $1,499 (64GB), $2,000 (128GB) - half the price of comparable M4 Pro Mac Mini ($2,399)

## Transcript

DJX Spark will be ready, will be
available shortly, probably in a few
weeks. DJX Spark is coming. It's always
been coming. But while we wait, GMK Tech
is not sitting around. They just dropped
the Evo X2, a mini PC with 128 GB of RAM
that's shared with the GPU, ready for
local LLMs and machine learning, and it
costs less than half the price of the
DJX Spark. Hey, maybe the Spark is out
by the time this video is edited. And in
that case, I'll leave my words. I've
ordered it also just uh to make sure I
test everything here. So, make sure you
don't miss that. And why would anybody
still care about the DJX Spark? I'll get
into that in a bit, too. Now, this EVO
is here and it's real sort of. They are
a little bit hard to get because it's
got that new AMD chip in it, but they're
trying to ramp up production because
it's so popular. It's a beast compared
to most of GMK Tech lineup. And it's
loaded with ports. two USB 4 ports, five
USBAports, SD card reader, HDMI display
port, two headset ports, and of course a
5 GB Ethernet. 5 GBs. We're up to that
now. Nice. But the real headline is
what's inside. That's AMD's new Ryzen AI
Max Plus 395. Yes, I got it right. 16
cores, 32 threads, and up to 5.1 GHz.
And the internal GPU that's supposed to
rival a 4060. Yeah, the RTX 460 is the
Radeon 8060S and they come in 64 GB.
This is the 128 GB version, which I'll
be testing right now for LLMs. That's
LPDDR5X, which is a pretty fast mobile
memory, 8,000 MHz. It also has dual PCIe
Gen 4 SSD slots inside. So, yeah,
upgradeable, too, up to 16 TB. And it's
got Wi-Fi 7,
which, you know, you might have that.
Some some people have it. I don't have
it. Of course, this PC can handle the
latest games without breaking a sweat.
Huh?
I said it can handle the latest games
without breaking a sweat. But what
really sets it apart is this APU. That's
that combination of the CPU and GPU on
the same chip with the unified memory
setup. Well, it's not exactly unified
memory. The memory can be shared, but
it's different from Apple Silicon's or
unified memory architecture. It's called
static partitioning. Thanks for that,
Casey. What does that mean for us? This
thing is built for running local LLMs,
just like I demoed in my members live
stream when I first got it in. You might
have also recently seen my video where I
test that same chip with 128 gigs on
this device. This is Asus's Flow Z13,
but the thermals on these machines are
very, very different. So, I wonder if
this thing is going to be pushing that
chip to its limit cuz the fans on this
thing are everywhere. By the way, Mac
minis, we're going to compare it to that
as well. Why? Because bang for the buck,
they do really well. Now, before we get
going, let's kick off one test just to
get a baseline of what this feels like.
I do have a bunch of models here, and
I'll go through the results uh in a bit.
And a very popular model is this Quinn
32 billion. This is the Q4. So, the
quantization is four. And LM Studio is
telling me I can offload 64 out of 64
layers. So, let's do that. Load model.
Let's make sure that this thing is in
fact getting loaded on the GPU. So,
let's check out task manager. It does
get loaded into memory first. system
memory. We're split up 64 and 64. 64 for
system, 64 for GPU. So, it gets copied
to the system memory first and then
copied over to the GPU memory. And
that's what this static partitioning of
memory is all about. It's not unified
memory like it is on Apple. We'll see
some differences in a bit. Model is
loaded. You can see it in dedicated GPU
memory right here. And we'll say hi. I
know, I know you all hate it when I say
hi, but it's one token and it gives the
best possible results. I've given the
model thousands of tokens before as
input and the results are just slightly
different. So it's not a huge deal as
far as the context size, but here we'll
get the most the fastest possible speed.
So we can compare something, right?
Because lots of people have different
kinds of prompts that they use. How do
you compare? Use the same prompt. 10.58
tokens per second here, which is pretty
reasonable for a 32 billion parameter
model that's 18.4 GB in size. Let's do
one more. a model that might not
necessarily fit into a RTX 4090 or even
an RTX 5090, which is this one right
here, Llama 3.370 billion Q4. It's 39 GB
on disk. And LM Studio is telling me I
should offload only 75 out of 80 layers
to the GPU. I'm going to push it all the
way to 80 and see what happens here. By
the way, if you're kind of new to this,
if you offload even any layers to the
CPU, things are just going to go
downhill really fast and become very,
very slow. You want everything to be on
the GPU. Well, it's not complaining yet.
Look at that spike in dedicated GPU
memory. We're up to 43 GB used on that
one. And it looks like it loaded. So,
I'm going to start a new chat. And let's
do a little high
there. It's going. It's going. It's not
as fast, but it's going. 5.1 tokens per
second here. And you can see that nice
spike on the GPU, which means it's fully
utilizing the GPU. That's good. The CPU
had spikes, but this is not the
processing. The processing is happening
on the GPU. So that's good. And we're
about 170 watts or there about 172 174.
That's the maximum that it's working at.
It is again set to best performance.
We'll switch over to the max for a
moment to take a look at the most
important factor in figuring out the
speed of these LLMs. Memory bandwidth.
And check out this site. What is this
from the '90s? I think it is actually.
Look at that. It's got borders. Remember
table borders? Anybody was around back
then? Anyway, this is called the stream
benchmark. And this has been around in
the industry for a long time measuring
memory bandwidth. Even on the AMD site,
they point to this benchmark. So, this
is good to run on all systems because I
compile it on that system. So, I
compiled this and I ran this on the Mac.
This right here is the M4 Mac Mini. And
according to Apple, the M4 chip has the
memory bandwidth of 120 GB per second.
Pretty good already. And that's just the
base model. So, right now, I measured
this running stream. And if we look at
this Triad number, we're getting 96 GB
per second or thereabout. It's not 120
like Apple advertises, but it's pretty
close. So, we're going to consider that
fact that it's just a little bit less
than the advertised number. So, here
I've got DeepSeek R1 distill quen 7
billion parameter model with a Q4
quantization. It's about 4 GB on disk.
Let's load that up. And everything is
going to the GPU here. Load it up pretty
quickly. And here we go. 21 tokens per
second. Let's keep an eye on the GPU
history here while I run another prompt.
And I'm going to have it write a little
bit of a longer output. And we are at 31
watts, which is actually pretty decent.
What am I saying? Decent. I mean, this
is pretty good, right? 31 watts. We got
19.94 tokens per second. And you can see
that the GPU was pretty much fully
utilized while that was going on. All
right. I know what you're going to say.
I didn't do the same model on the GMK
tech. Let's do
that. But I'm going to tell you right
now, it's going to be much faster. Yes,
it sure is. And the result is 43 tokens
per second here. Why is that? This
machine is so much faster than the M4
Mac Mini with this. Hm. We can stop
there. I mean, now you know, but we're
going to get a little bit more nerdy
here. So, these days, I'm constantly
flipping between models. GPT4 for notes
and email, claude for code refactors,
Flux for image generation, cling for
video, four tabs, four bills, and
counting. Enter Chat LLM Teams. There's
one dashboard that houses every top LLM
and route LLM picks the right one for
you for a given task. 04 mini high for
fast answers. Claw sonnet 3.7 for
coding. Gemini 2.5 Pro for big context
and even adds GPT 4.1 before Chad GPT
has it. Chat with PDFs and PowerPoints,
then generate decks and docs and do deep
research all in the same chat. Need
human sounding copy? The humanized
toggle rewrites text to beat AI
detectors. Spin up agents and run code
with AI engineer. I built my first bot
in just minutes. Track artifacts. Create
GitHub pull requests and debug from the
same interface. Need visuals? No
problem. Use Flux or Ideoggram and
Recraft for images. Cling, Luma, and
Runway for video, all builtin. And the
kicker is just $10 a month, less than
one premium model. Head over to
chatlm.abacus.ai AI or click the link in
the description and level up with Chat
LLM Teams. Now, a lot of you might
already know that there is something
called Llama CPP which is the backbone
on what LM Studio runs on. And Llama CPP
when compiled for the platform, you can
compile it uh for Rockom, you can
compile it for Vulcan, you can compile
it for the CPU only. There's just
different optimizations and different
libraries you could include when you're
compiling Llama CPP. If we take a look
at LM Studio and the settings here
behind the scenes, you can take a look
at runtimes. We've got CPU llama CPP
which is available here, but also Vulcan
llama CPP. And Vulcan is a crossplatform
kind of library that uh basically allows
you to use this on Linux, use this on
Windows, it'll work on an Intel machine,
it'll work on AMD machine, but there's a
little bit of an overhead to that to
doing it that way. the crossplatform
approach. The more direct approach is
something called Rockom for AMD
processors. Unfortunately, Rockom is not
available and I'll come back to that.
You can see that here. There's more to
that story though and Rockom support is
coming soon, but I'll talk about that
momentarily. Llama CPP, jumping back to
that, they give you a bunch of tools
that you can use including something
called Llama. So, I'm going to go to my
users folder, LM Studio models, and I'll
pick one here. So, let's go with LM
Studio Community. This DeepSseek R1
distill is good. And there's GGUF models
in here. So, I'm going to grab that Q4
one and pass it to
LMBench. And boom. And you can see here
it is built with Vulcan. So, this is
going to utilize Vulcan, which is good.
This gives us two result. One is PP512.
I know it's a funny funny word. Silly
jokes. This gives us the prompt
processing, which is what PP stands for.
And this is good. 935 tokens per second.
That's nice. And we're getting TG128. TG
is token generation and that's 46 tokens
per second. So, we're kind of close to
what LM Studio gives us. 43 there. 46
Lama Bench. Just to give you an idea how
far off we are. And by the way, this is
within the margin of error. I can also
use Llama Bench on other machines like
the Mac Mini because I compiled Llama
CPP there as well. Definitely good to be
in the nerdy section right now. I sound
way more nerdy because I got a little
bit of a cold. Running Llama Bench with
the same model on the M4 Mac Mini, we
get a much much lower result. 228 for
PP512 and TG128 gets us 21 tokens per
second, which uh kind of aligns with
what we've seen in LM Studio on the Mac
as well. Now, why is this happening? I
thought the M4s were supposed to be
amazing, but I do have one more thing to
show you, which is this Mac Mini. This
is the M4 Pro Mac Mini. And some people
think, "Oh, M4 Pro, right? It's just uh
more cores, right?" No, that's not the
only difference. Yes, it does have more
cores. And the M4 Max, which is not
available in a Mac Mini, so I have this
that has more cores, too. But the thing
that's going to matter most when it
comes to this kind of work, machine
learning, is memory bandwidth. And here
is our result for that. Same model.
We're getting about double. Let's
confirm that in LM Studio. And yeah, 44
tokens per second for our high prompt.
So what gives? Is it just a matter of
getting more cores? Well, the way Apple
scales our chips is they have the single
M4 core. And if you run Geekbench on
these, you'll see that the single core
result is pretty much the same across
the M4 family. The multi-core score is
going to be different, but it scales
pretty much linearly. The extra thing
that you're going to get is more GPU
cores and also faster memory bandwidth.
For the M4, it's 120 GB per second. For
the M4 Pro, it's 273 GB per second. For
the M4 Max, it's 540 GB per second or
somewhere around there. The M3 Ultra,
819 GB per second. So, given that we
know those numbers, the Ryzen AI Max
Plus chip has a memory bandwidth,
published memory bandwidth of 256 GB per
second. Where does that put us? Well,
it's much faster than the M4, but it's
about 20 GB slower than the M4 Pro. And
where's Nvidia's DJX Spark on that?
Well, that's at 279. So, slightly higher
memory bandwidth than the M4 Pro. We'll
see what happens when we actually get
that in and test it out. Since I'm doing
the M4 Pro right now, let's check out
its power usage, by the way. And we're
at 47 watts. So, quite a bit uh more
than the base M4, but nowhere close to
the power used by this machine. Now,
since we're talking about memory
bandwidth, let's take a look at the
measurements I actually got using the
Stream benchmark. Over here on the M4, I
got 96 GB out of the 120 that Apple
advertises. It's not exactly close to
that, but it's good to know that that's
how it's measured. So, whatever is going
on in the stream benchmark and the way
it's measuring it, we're getting a
little bit of a loss there. M4 Pro, not
bad. 209 out of the 273 that Apple says.
What about this machine? Let's take a
look at that. Let's run this one here.
And we're getting 120. 120 is way way
lower than I would expect because we
wanted 256, right? And there's a bunch
of machines coming out with this chip
that are going to have the same kind of
results here. The framework desktop is
one of them. Beink has a couple of
machines coming out. The ASUS and
finally the first desktop is this one,
but 120 is way low. What's going on?
Well, it's either the benchmark uh is
somehow not running the way it's
supposed to on this machine. And I reran
this multiple times of course. But that
tells me that if we are getting such
high numbers for Llama Bench through
Vulcan, which is really really nice, 935
and 46, which is even better than the M4
Pro. And if the stream benchmark is
correct at 120, that means that this
chip may be able to be pushed further.
I'm not sure about that yet, but maybe.
And if it is able to be pushed further,
maybe it can be even faster. But for
now, for practical reasons, results is
what matters. And we're getting good
numbers here from this machine. Gemma 3
1 billion. We're getting 100 almost 160
tokens per second here. Llama 3.2 1
billion. Instruct 130. Gemma 3 4
billion. We're getting 66 tokens per
second. Here are the Deep Seek Distills.
We've seen 44. Gemma 312 billion. Now
we're getting into really more useful
models. 23.9 still a really decent
score. 14 billion 23.7. Quen 2.5 quarter
32 billion. 32 billion parameter models
are actually pretty useful. We're
getting 10.8. Not the best. I'd rather
be over 20, but still 10.8 is
reasonable. And finally, Llama 3.370
billion Q4 five tokens per second. It
runs and it runs well, but that's
getting into a territory where it's a
little bit too slow. Now, here's just an
interesting tidbit. Uh, comparing
desktop to the laptop version of the
same thing. This is a very, you know,
thermally limited machine. It does a
really good job, but still it's a laptop
tablet. And you can see that here. This
is the results for the Knuckbox Evo X2,
which is this, versus that FlowZ13 with
128 gigs. And you can see the little
green dots are just a little bit higher
than the orange dots for each one of
these, except for this one. Gemma 312
billion. The laptop beats the desktop
for some reason on that one. And by the
way, you'll see that uh I only ran the
64 GB configuration here for the GPU
because that's kind of an even split
between the two. I can load really large
models that way. And I have a slightly
different view here that compares a
bunch of different machines. The FL Z13
with 32 gigs. We've got the Nugbox Evo
here. This is Gemma 3 12 billion
parameter model, 23 tokens per second.
And here's the M4 Max machine with 49
tokens per second for that same model.
So quite a bit faster. But again, that's
the memory bandwidth showing up. Getting
back to why we have these very different
machines coming up, we have this very
unique chip. We've got the Applebased
architecture which is unified memory
architecture. And we've got this Nvidia
Spark coming in a bunch of different
varieties of machines. There's going to
be the Asus machine, the Dell, bunch of
partners are going to have that uh chip
inside of it, the Blackwell chip. That
one has 278 slightly higher memory
bandwidth than the M4 Pro and the Ryzen
AI Max Plus 395. We'll see how that
translates. And I'm also not sure how
the memory partitioning is going to work
in that machine. But here we do have
static partitioning which is fixed and
of course that's a way of dividing
memory into set blocks before the
computer starts up. So you kind of have
to know ahead of time. What's nice about
that is easier to set up, uses few
system resources and keeps each program
in its own space which makes things a
little bit more stable. For example, the
same chip here using Armory Crate I was
able to set to auto and that gave us
pretty bad results when it comes to
LLMs. probably due to the extra overhead
of moving things around in memory when
you have to allocate certain amount of
space to different processes. But static
partitioning also wastes memory if a
program doesn't use all that space in a
block. Big programs that don't fit in a
block can't run and only limited number
of programs can run at the same time.
It's okay for simple systems, but not
great for modern computers that need
more flexibility. That said, will this
be a good little sidekick machine with
AI support and dev? Yes, it will be
great and it's going to get even better.
According to AMD, this is going to
significantly improve in theory. I tried
getting Rockom to working with a special
patched Olama version for a while with
no success. AMD is working on supporting
Rockom on more of their offerings
besides a few of the top discrete GPUs.
However, during Computex, they did say
they would pay more attention to the LLM
community. Hey, that's us. and get
Rockom support in more chips, including
this one, without having to hunt down
and monkey patch a bunch of versions
with random pieces of software across
multiple obscure GitHub repositories.
You'll see a lot of people trying it on
this chip and with limited success. For
example, Michael Larabel from Feronx got
this working as long as you use Rockom
6.4.1 and install Ubuntu 24.04, 04 not
25 sticking with the Linux 6.11 HWE
kernel and avoid trying to build DKMS
modules on newer kernels like 6.14 and
even then Blender with HIP backend was
unstable and Llama CPP triggered
segmentation faults so it's still not
super stable but he says hopefully later
in the year we're close so it'll be
fantastic if AMD follows through and
I'll be sure to test that support when
it comes out for now we have Vulcan
which does a pretty decent job. Now,
what about the price? This thing is not
your average cheap nuck. These things
are getting serious. Mini PCs are
starting to get real value, real chips
because our chips are shrinking and
still giving us a lot of power. While it
is a mini PC, it's kind of in between a
mini PC and a desktop. It's bigger and
the price is a little bit bigger, too.
$14.99 for the 64 GB version and 2 grand
for the 128 GB version that I have here.
There is a crazy amount of demand for
this particular chip right now. If you
ask me if this is an expensive price to
pay for this machine, I would say no
because this machine is a beast and a
powerhouse in a tiny package. Now, if
you compare it to the M4 Mac Mini, you
saw that the performance of the Mac Mini
is good, but because of the memory
bandwidth limitations, it's not going to
be as powerful as this thing. Also, the
number of cores is much less, so it's
going to be less powerful. Also, 64 and
128 gigs of RAM. You don't get that with
the M4 Mac Mini. You can get 64 with the
M4 Pro. That's this one right here. But
then this machine cost 23.99. So quite a
bit more. All right, that's it folks for
this one. Hope you enjoyed this video.
If you did, give it a thumbs up.
Consider joining the channel as a
member. I did actually run a few tests
for members in a live stream when I
first got this machine in. So you can
sign up down below as well for that.
Otherwise, you might be interested in
this video right over here. Thanks for
watching and I'll see you next time.
[Music]
